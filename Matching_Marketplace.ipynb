{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект: Определение похожих товаров с использованием FAISS и оценка качества рекомендаций"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Описание проекта:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Цель проекта"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создайть систему, способную идентифицировать и рекомендовать пять товаров, наиболее схожих с каждым элементом из проверочного датасета (validation.csv), используя для этого основную базу данных товаров (base.csv). Эффективность и точность предлагаемых рекомендаций необходимо оценить по метрике accuracy@5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Используемые технологии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* FAISS для быстрого поиска по схожести в больших наборах данных.\n",
    "* Scikit-learn для обучения моделей и оценки качества рекомендаций."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### План работы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Шаг 1: Подготовка данных и анализ\n",
    "  * Загрузка и первичный анализ данных из base.csv, validation.csv и validation_answer.csv.\n",
    "  * Преобразование идентификаторов Id и Target из строк в числовые значения для упрощения обработки.\n",
    "  * Разделение признаков и целевых переменных, нормализация данных.\n",
    "* Шаг 2: Работа с FAISS\n",
    "  * Создание и конфигурация GPU-оптимизированного индекса в FAISS для базового набора данных.\n",
    "  * Поиск топ-5 похожих товаров для каждого элемента в валидационном наборе.\n",
    "* Шаг 3: Подготовка обучающего датасета\n",
    "  * Формирование обучающего датасета на основе результатов поиска с использованием FAISS, включая создание признаков, отражающих схожесть между товарами.\n",
    "  * Определение целевой переменной (Target) на основе правильных ответов из validation_answer.csv.\n",
    "* Шаг 4: Обучение модей\n",
    "  * Оценка качества модели с использованием кросс-валидации и метрики accuracy@5.\n",
    "* Шаг 5: Анализ результатов\n",
    "  * Детальный анализ результатов предсказания модели.\n",
    "  * Сравнение эффективности рекомендаций FAISS и точности классификации модели логистической регрессии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка библиотек, таблиц, инициализация функций и констант"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Базовые библиотеки для работы с данными\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Визуализация\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Модели и инструменты машинного обучения\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "# Утилиты\n",
    "from IPython.display import display\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "# Библиотеки для анализа данных\n",
    "import phik\n",
    "from phik import report\n",
    "\n",
    "# Игнорирование предупреждений\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.min_rows', 15)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.options.display.float_format = '{:,.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = pd.read_csv('validation.csv')\n",
    "\n",
    "valid_target= pd.read_csv('validation_answer.csv')\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "\n",
    "base = pd.read_csv('base.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция информации по таблице\n",
    "def dataframe_summary(df, string):\n",
    "    # Вывод общей информации\n",
    "    print(\"Общая информация по таблице:\", string)\n",
    "    df.info()\n",
    "\n",
    "    print(\"\\n Статистическое описание:\")\n",
    "    display(df.describe().transpose())\n",
    "\n",
    "    print(\"\\nСлучайные примеры:\")\n",
    "    display(df.sample(5))\n",
    "\n",
    "    print(\"\\nКоличество строк и столбцов:\", df.shape)\n",
    "\n",
    "    print(\"\\nКоличество явных дубликатов:\", df.duplicated().sum())\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downcast_dataframe(df):\n",
    "\n",
    "    # Цикл по всем столбцам DataFrame\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'float64':  # Проверяем, является ли тип столбца float64\n",
    "            df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "        elif df[col].dtype == 'int64':  # Проверяем, является ли тип столбца int64\n",
    "            df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все данные загружены, можно приступать к работе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обзор данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for data in [valid, valid_target, train, base]:\n",
    "#    dataframe_summary(data, 'Данные')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* В данных отсутсвуют пропуски и дубликаты. \n",
    "* Наблюдается сильный разброс значений, понадобится масштабирование.\n",
    "* Стоит отменить, что в `valid_target` в колонке `Expected` 91502 уникальных значений из 100000 строк. Значит есть товары, которые указали как схожие несколько раз."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уменьшим потребляемый объем памяти данных. Переведем столбец Id в формат индекса. И уменьшим регистр называний столбцов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = downcast_dataframe(base)\n",
    "base.columns = base.columns.str.lower()\n",
    "base.set_index('id', inplace=True)\n",
    "\n",
    "train = downcast_dataframe(train)\n",
    "train.columns = train.columns.str.lower()\n",
    "train.set_index('id', inplace=True)\n",
    "\n",
    "valid = downcast_dataframe(valid)\n",
    "valid.columns = valid.columns.str.lower()\n",
    "valid.set_index('id', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_target = downcast_dataframe(valid_target)\n",
    "valid_target.columns = valid_target.columns.str.lower()\n",
    "valid_target.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 100000 entries, 0-query to 99999-query\n",
      "Data columns (total 73 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   0       100000 non-null  float32\n",
      " 1   1       100000 non-null  float32\n",
      " 2   2       100000 non-null  float32\n",
      " 3   3       100000 non-null  float32\n",
      " 4   4       100000 non-null  float32\n",
      " 5   5       100000 non-null  float32\n",
      " 6   6       100000 non-null  float32\n",
      " 7   7       100000 non-null  float32\n",
      " 8   8       100000 non-null  float32\n",
      " 9   9       100000 non-null  float32\n",
      " 10  10      100000 non-null  float32\n",
      " 11  11      100000 non-null  float32\n",
      " 12  12      100000 non-null  float32\n",
      " 13  13      100000 non-null  float32\n",
      " 14  14      100000 non-null  float32\n",
      " 15  15      100000 non-null  float32\n",
      " 16  16      100000 non-null  float32\n",
      " 17  17      100000 non-null  float32\n",
      " 18  18      100000 non-null  float32\n",
      " 19  19      100000 non-null  float32\n",
      " 20  20      100000 non-null  float32\n",
      " 21  21      100000 non-null  float32\n",
      " 22  22      100000 non-null  float32\n",
      " 23  23      100000 non-null  float32\n",
      " 24  24      100000 non-null  float32\n",
      " 25  25      100000 non-null  float32\n",
      " 26  26      100000 non-null  float32\n",
      " 27  27      100000 non-null  float32\n",
      " 28  28      100000 non-null  float32\n",
      " 29  29      100000 non-null  float32\n",
      " 30  30      100000 non-null  float32\n",
      " 31  31      100000 non-null  float32\n",
      " 32  32      100000 non-null  float32\n",
      " 33  33      100000 non-null  float32\n",
      " 34  34      100000 non-null  float32\n",
      " 35  35      100000 non-null  float32\n",
      " 36  36      100000 non-null  float32\n",
      " 37  37      100000 non-null  float32\n",
      " 38  38      100000 non-null  float32\n",
      " 39  39      100000 non-null  float32\n",
      " 40  40      100000 non-null  float32\n",
      " 41  41      100000 non-null  float32\n",
      " 42  42      100000 non-null  float32\n",
      " 43  43      100000 non-null  float32\n",
      " 44  44      100000 non-null  float32\n",
      " 45  45      100000 non-null  float32\n",
      " 46  46      100000 non-null  float32\n",
      " 47  47      100000 non-null  float32\n",
      " 48  48      100000 non-null  float32\n",
      " 49  49      100000 non-null  float32\n",
      " 50  50      100000 non-null  float32\n",
      " 51  51      100000 non-null  float32\n",
      " 52  52      100000 non-null  float32\n",
      " 53  53      100000 non-null  float32\n",
      " 54  54      100000 non-null  float32\n",
      " 55  55      100000 non-null  float32\n",
      " 56  56      100000 non-null  float32\n",
      " 57  57      100000 non-null  float32\n",
      " 58  58      100000 non-null  float32\n",
      " 59  59      100000 non-null  float32\n",
      " 60  60      100000 non-null  float32\n",
      " 61  61      100000 non-null  float32\n",
      " 62  62      100000 non-null  float32\n",
      " 63  63      100000 non-null  float32\n",
      " 64  64      100000 non-null  float32\n",
      " 65  65      100000 non-null  float32\n",
      " 66  66      100000 non-null  float32\n",
      " 67  67      100000 non-null  float32\n",
      " 68  68      100000 non-null  float32\n",
      " 69  69      100000 non-null  float32\n",
      " 70  70      100000 non-null  float32\n",
      " 71  71      100000 non-null  float32\n",
      " 72  target  100000 non-null  object \n",
      "dtypes: float32(72), object(1)\n",
      "memory usage: 29.0+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Потребляемый объем памяти упал почти в два раза."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выделение цел. признака из обучающей выборки\n",
    "train_target = train['target']\n",
    "train.drop('target', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Отбор 30% данных из base случайным образом\n",
    "sampled_base_df = base.sample(frac=0.3, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# словарь id и номеров векторов\n",
    "base_index = {k: v for k, v in enumerate(base.index.to_list())}\n",
    "train_index = {k: v for k, v in enumerate(train.index.to_list())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_cols = base.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.cuda as cuda\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.version.cuda)  # Выводит версию CUDA, с которой скомпилирован PyTorch\n",
    "print(torch.cuda.get_device_name(0))  # Выводит имя вашего GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализация масштабировщика\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Обучение масштабировщика на данных из base\n",
    "scaler.fit(base)\n",
    "\n",
    "# Масштабирование данных base, train и valid\n",
    "base = scaler.transform(base)\n",
    "train = scaler.transform(train)\n",
    "valid = scaler.transform(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.1592164 ,  0.6203504 , -0.5137226 , ..., -0.0140508 ,\n",
       "         1.7814199 , -0.31232867],\n",
       "       [ 2.0757148 ,  1.0604233 , -0.65249103, ...,  0.05984759,\n",
       "         1.8537259 , -0.28105187],\n",
       "       [ 1.2854173 , -0.3433421 ,  0.39787757, ...,  0.04852088,\n",
       "        -0.71384674,  0.36562327],\n",
       "       ...,\n",
       "       [-0.4337762 , -2.064035  , -0.69096935, ...,  0.6597316 ,\n",
       "        -0.71384674,  1.2577736 ],\n",
       "       [-0.02446471,  0.16793932,  0.25220424, ...,  0.43807355,\n",
       "        -0.71384674, -0.19157949],\n",
       "       [-0.6321803 ,  0.96487993, -0.17634065, ..., -1.0735649 ,\n",
       "        -0.7121896 ,  1.4986584 ]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определяем количество кластеров\n",
    "N_CLUSTERS = 200  # Пример количества кластеров\n",
    "\n",
    "d = base.shape[1]  # Размерность векторов признаков\n",
    "\n",
    "# Создание квантизатора для разделения векторного пространства на кластеры\n",
    "quantizer = faiss.IndexFlatL2(d)\n",
    "\n",
    "# Создание индекса IndexIVFFlat с использованием квантизатора\n",
    "idx_l2 = faiss.IndexIVFFlat(quantizer, d, N_CLUSTERS, faiss.METRIC_L2)\n",
    "\n",
    "# Инициализация ресурсов GPU\n",
    "res = faiss.StandardGpuResources()\n",
    "\n",
    "# Обучение индекса на базовом наборе данных\n",
    "if not idx_l2.is_trained:\n",
    "    idx_l2.train(base)\n",
    "\n",
    "# Создание GPU-версии индекса из CPU-версии\n",
    "gpu_index = faiss.index_cpu_to_gpu(res, 0, idx_l2)\n",
    "\n",
    "# Добавление данных в индекс\n",
    "gpu_index.add(base)\n",
    "\n",
    "# Поиск ближайших соседей для данных из valid\n",
    "k = 40  # Количество ближайших соседей для поиска\n",
    "D, I = gpu_index.search(valid, k)  # D - расстояния, I - индексы ближайших соседей\n",
    "\n",
    "# Вывод индексов ближайших соседей и соответствующих расстояний\n",
    "print(\"Индексы ближайших соседей:\\n\", I)\n",
    "print(\"Расстояния до ближайших соседей:\\n\", D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание индекса с использованием GPU\n",
    "d = base.shape[1]  # Размерность векторов\n",
    "res = faiss.StandardGpuResources()  # инициализация ресурсов GPU\n",
    "index = faiss.IndexFlatL2(d)  # создание L2 индекса\n",
    "gpu_index = faiss.index_cpu_to_gpu(res, 0, index)  # перенос индекса на GPU\n",
    "\n",
    "# Добавление данных в индекс\n",
    "gpu_index.add(base)\n",
    "\n",
    "# Поиск 5 ближайших соседей для первых 10 векторов\n",
    "k = 400\n",
    "D, I = gpu_index.search(base[:10], k)\n",
    "\n",
    "print(\"Индексы ближайших соседей для первых 10 векторов:\\n\", I)\n",
    "print(\"Расстояния до ближайших соседей для первых 10 векторов:\\n\", D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определение размерности векторов\n",
    "d = base.shape[1]\n",
    "\n",
    "# Создание индекса для использования на GPU\n",
    "res = faiss.StandardGpuResources()  # Инициализация ресурсов GPU\n",
    "index = faiss.IndexFlatL2(d)  # Создание индекса L2\n",
    "gpu_index = faiss.index_cpu_to_gpu(res, 0, index)  # Перемещение индекса на GPU\n",
    "\n",
    "# Добавление данных из base в индекс\n",
    "gpu_index.add(base)\n",
    "\n",
    "# Поиск 5 ближайших соседей для каждого вектора из valid\n",
    "k = 5  # Количество ближайших соседей для поиска\n",
    "D, I = gpu_index.search(valid, k)  # D - расстояния, I - индексы ближайших соседей\n",
    "\n",
    "# Теперь в I содержатся индексы ближайших соседей из base для каждого элемента из valid\n",
    "print(\"Индексы ближайших соседей:\\n\", I)\n",
    "print(\"Расстояния до ближайших соседей:\\n\", D)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
